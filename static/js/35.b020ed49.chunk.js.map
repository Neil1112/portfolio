{"version":3,"file":"static/js/35.b020ed49.chunk.js","mappings":"4eAOaA,EAAe,ggCAAAC,OAWlBC,EAAO,wBAAAD,O,qnYACiB,qaAAAA,OAOTE,EAAoB,mDAAAF,OAGvBG,EAAiB,yCAAAH,OAGtBI,EAAO,iFAAAJ,OAGaK,EAAgC,8BAMxDC,EAAU,mkiB","sources":["markdown-strings/flan-t5-fine-tuning.js"],"sourcesContent":["import ZeroShotInferenceImg from '../assets/flan-t5-fine-tuning/zero-shot-inference.png'\nimport FullFineTuningImg from '../assets/flan-t5-fine-tuning/full-fine-tuned.png'\nimport PEFTImg from '../assets/flan-t5-fine-tuning/peft.png'\nimport AbsolutePercentageImprovementImg from '../assets/flan-t5-fine-tuning/absolute-percentages.png'\nimport LoraImg from '../assets/flan-t5-fine-tuning/lora.png'\nimport LoraMatricesImg from '../assets/flan-t5-fine-tuning/lora-matrices.png'\n\nexport const markdownContent = `\n# FLAN-T5 Fine Tuning for Dialogue Summarisation\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FHY4tDWpdi-dva72LdARIyBD4CXot9wO?usp=sharing)\n\n\nI fine tuned FLAN-T5 from Hugging Face for enhanced dialogue summarisation. The FLAN-T5 provides a high quality instruction tuned model and can summarise text out of the box. To improve the inferences, I applied Parameter Efficient Fine-Tuning (PEFT), particularly the Low Rank Adapter (LoRA) technique. I performed both quantitative (human) and qualitative evaluation using ROUGE metrics.\n\n## Parameter Efficient Fine-Tuning (PEFT)\nI used Low Rank Adapters (LoRA) to perform the fine-tuning. LoRA introduces two low rank matrices. Instead of the training the full model weights, we train these low rank matrices to avoid high computational costs. The low rank matrices are then used to compute the full model weights. This technique is called Parameter Efficient Fine-Tuning (PEFT).\n\n![LoRA](${LoraImg})\n![LoRA Matrices](${LoraMatricesImg})\n\n\n## Zero Shot Inference Vs Full Fine Tuning Vs PEFT (LoRA)\nI compared the performance of the three models using the ROUGE metric. The results are shown below. As we can see, both Fully Fine Tuned and the PEFT model outperforms the base model. And the PEFT model while saving a lot of time and resources, performs almost as good as the fully fine tuned model.\n\n### Zero Shot Inference\n![Zero Shot Inference](${ZeroShotInferenceImg})\n\n### Full Fine Tuning\n![Full Fine Tuning](${FullFineTuningImg})\n\n### PEFT (LoRA)\n![PEFT (LoRA)](${PEFTImg})\n\n### Absolute Percentage Improvement\n![Absolute Percentage Improvement](${AbsolutePercentageImprovementImg})\n\n\n## Implementation\n`\n\nexport const codeString = `%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet\n\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np\n\n\n# loading the dialogsum dataset from hugging face\n# it contains 10,000+ dialogues with the corresponding manually labelled summaries and topics.\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\n\ndataset = load_dataset(huggingface_dataset_name)\n\ndataset\n\n\n\n# loading the pre-trained FLAN-T5 model and its tokenizer from hugging face\nmodel_name='google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n# utility function to help know the number of trainable parameters\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))\n\n\n# Testing Zero Shot Inferencing\nindex = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n\n\n# Perfoming Full-tuning\n# Preprocess the Dialog-Summary Dataset\ndef tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n\n\n# subsample of the dataset to save some time\ntokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)\n\n\n# Fine tuning the Dataset\noutput_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_steps=1,\n    max_steps=1\n)\n\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)\n\ntrainer.train()\n\n# Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.\n!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/\n\ninstruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)\n\n\n# Evaluate the Model Qualitatively (Human Evaluation)\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n\n\n# Evaluate the Model Quantitatively (with ROUGE Metric)\nrouge = evaluate.load('rouge')\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf\n\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\n\n\n# viewing pre-populated list of model results\nresults = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\n\n\n# show percentage improvements\nprint(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')\n\n\n    # Perfoming Parameter Efficient Fine-Tuning (PEFT)\n    # setup the PEFT/LORA model for fine-tuning\n    from peft import LoraConfig, get_peft_model, TaskType\n    \n    lora_config = LoraConfig(\n        r=32, # Rank\n        lora_alpha=32,\n        target_modules=[\"q\", \"v\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n    )\n    \n    # adding LoRA adapter layers/parameters to the original LLM\n    peft_model = get_peft_model(original_model, \n                                lora_config)\n    print(print_number_of_trainable_model_parameters(peft_model))\n\n\n    # training PEFT adapter\n    output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n    \n    peft_training_args = TrainingArguments(\n        output_dir=output_dir,\n        auto_find_batch_size=True,\n        learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n        num_train_epochs=1,\n        logging_steps=1,\n        max_steps=1    \n    )\n        \n    peft_trainer = Trainer(\n        model=peft_model,\n        args=peft_training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n    )\n    \n    peft_trainer.train()\n    \n    peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n    \n    peft_trainer.model.save_pretrained(peft_model_path)\n    tokenizer.save_pretrained(peft_model_path)\n    \n    # loading a model from checkpoint that was trained on full dataset\n    !aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/\n    \n    # preparing for inference\n    from peft import PeftModel, PeftConfig\n    \n    peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n    \n    peft_model = PeftModel.from_pretrained(peft_model_base, \n                                           './peft-dialogue-summary-checkpoint-from-s3/', \n                                           torch_dtype=torch.bfloat16,\n                                           is_trainable=False)\n    \n    print(print_number_of_trainable_model_parameters(peft_model))\n\n\n# Evaluate the Model Qualitatively (Human Evaluation)\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')\n\n\n# Evaluate the Model Qualitatively (with ROUGE Metric)\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf\n\n\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n\n\nprint(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')\n\n\nprint(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')\n`"],"names":["markdownContent","concat","LoraImg","ZeroShotInferenceImg","FullFineTuningImg","PEFTImg","AbsolutePercentageImprovementImg","codeString"],"sourceRoot":""}