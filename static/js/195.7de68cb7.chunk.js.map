{"version":3,"file":"static/js/195.7de68cb7.chunk.js","mappings":"sPAEaA,EAAe,+PAAAC,OAQhBC,EAAM,8BAMLC,EAAU,4sH","sources":["markdown-strings/gpt-knowledge-extension.js"],"sourcesContent":["import RAGImg from '../assets/gpt-knowledge-extension/rag.jpg'\n\nexport const markdownContent = `# GPT Knowledge Extension\n\nI extended the knowledge of GPT-3.5 using Retrieval Augmented Generation (RAG) method using Langchain and ChromaDB as Vector database.\n\n[Try Demo](https://neil1112-umang-ai-app-wyvnyh.streamlit.app)\n\n<br/>\n\n![RAGImg](${RAGImg})\n\n\n## Implementation\n`\n\nexport const codeString = `import streamlit as st\nfrom langchain.llms import OpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\nfrom dotenv import load_dotenv\nimport os\nimport nltk\nimport nltk.data\nfrom io import StringIO\nimport time\nfrom streamlit_chat import message\nimport uuid\n\n# local imports\nfrom utils.chatInterface import initiateChat, getInput, getResponse\n\n\n# # open ai key\n# load_dotenv()\n# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nOPENAI_API_KEY = st.secrets[\"OPENAI_API_KEY\"]\n\n# download nltk punkt\n# download_dir = os.path.join('./', 'nltk_data')\n# nltk.data.load(\n#     os.path.join(download_dir, 'tokenizers/punkt/english.pickle')\n# )\n\n# with st.sidebar:\n#     openai_api_key = st.text_input('OpenAI API Key')\n\n\nst.title('Umang.ai')\nst.write('A collection of NLP tools for the Umang.ai project.')\n\nst.info('Explore different information from the sidebar. \\n')\nst.subheader('Things to try:')\nst.write('- Upload a text file')\nst.write('- Ask long and short queries on it.')\nst.write('- Compare the results of these queries. It will help us develop the Prompt Template to fine tune the queries.')\nst.write('- Ask for brief vs detailed answers.')\nst.write('- Also compare the results with pure GPT-4. It can be accessed from the sidebar.')\n# create a file uploader\nuploaded_file = st.file_uploader(\"Upload a file\", type=\"txt\")\n\n\nif uploaded_file is not None:\n    # read byte data\n    bytes_data = uploaded_file.getvalue()\n    # st.write(bytes_data)\n\n    # convert to string\n    stringio = StringIO(bytes_data.decode('utf-8'))\n    # st.write(stringio)\n\n    # read file as string\n    document = stringio.read()\n    # st.write(document)\n\n    # split into chunks\n    with st.spinner('Splitting Documents...'):\n        time.sleep(1)\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n    texts = text_splitter.create_documents([document])\n\n    st.info('Splitted into {} documents'.format(len(texts)))\n    # st.info('Document 1 {}'.format(texts[0]))\n\n    # embeddings\n    with st.spinner('Creating Embeddings...'):\n        time.sleep(1)\n    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n\n    # create a vectorstore to use as index\n    persist_directory = \"db\"\n    with st.spinner('Creating Vectorstore...'):\n        # db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n        vectordb = Chroma.from_documents(texts, embeddings)\n\n    # loading the vectorstore\n    with st.spinner('Loading Vectorstore...'):\n        time.sleep(1)\n    # vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n    \n    # expose index in a retriever interface\n    retriever = vectordb.as_retriever()\n\n    # create a chain to answer questions\n    qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_key=OPENAI_API_KEY), retriever=retriever, chain_type=\"stuff\")\n\n    # create the query input form\n    # with st.form(\"query_input\"):\n    #     query = st.text_input(\"Enter your query here\", key=\"query\")\n    #     submitted = st.form_submit_button(\"Submit\")\n    #     if submitted:\n    #         with st.spinner(\"Generating answer...\"):\n    #             answer = qa.run(query)\n    #             st.write(answer)\n\n    # initiate chat\n    initiateChat()\n\n\n    # get input from user\n    user_input = getInput()\n\n    # get response\n    if user_input is not None:\n        getResponse(user_input, qa)\n\n    # show messages\n    # finalResponse(user_input)\n`"],"names":["markdownContent","concat","RAGImg","codeString"],"sourceRoot":""}