{"version":3,"file":"static/js/569.2a1156ca.chunk.js","mappings":"yqBAUaA,EAAe,uuBAAAC,OAgBOC,EAA6B,iPAAAD,OAKlDE,EAAQ,6mBAAAF,OAcRG,EAAQ,uIAAAH,OAOEI,EAAkB,wBAAAJ,OACvBK,EAAa,qBAAAL,OAElBM,EAAQ,mKAAAN,OAKRO,EAAQ,2UAAAP,OAMRQ,EAAQ,8KAAAR,OAMRS,EAAQ,0bAsBTC,EAAU,i1S","sources":["markdown-strings/neural-style-transfer.js"],"sourcesContent":["import NeuralStyleTransferConceptImg from '../assets/neural-style-transfer/neural-style-transfer-concept.png'\nimport FilterUnRollingImg from '../assets/neural-style-transfer/filter-unrolling.png'\nimport GramMatrixImg from '../assets/neural-style-transfer/gram-matrix.png'\nimport Math1Img from '../assets/neural-style-transfer/1.png'\nimport Math2Img from '../assets/neural-style-transfer/2.png'\nimport Math3Img from '../assets/neural-style-transfer/3.png'\nimport Math4Img from '../assets/neural-style-transfer/4.png'\nimport Math5Img from '../assets/neural-style-transfer/5.png'\nimport Math6Img from '../assets/neural-style-transfer/6.png'\n\nexport const markdownContent = `# Neural Style Transfer\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Wl1Dcn10Q7aAymjSmhypX6C9FJ9GDQEE?usp=sharing)\n\n\nNeural Style Transfer (NST) merges two images, namely a ‚Äúcontent‚Äù image (C) and a ‚Äústyle‚Äù image (S), to create a ‚Äúgenerated‚Äù image (G). It uses a previously trained convolutional network, and builds on top of that. This idea of using a network trained on a different task and applying it to a new task is called transfer learning.\n\n<iframe\n\tsrc=\"https://lsquaremaster-neural-style-transfer.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"100%\"\n\theight=\"550px\"\n></iframe>\n\n<br/>\n\n![NeuralStyleTransferConceptImg](${NeuralStyleTransferConceptImg})\n\n\nI have used VGG-19 network which was trained on the very large ImageNet database, and has learned to recognise a variety of low level features(at the shallower levels) and high level features(at the deeper levels).\n\n![Math1Img](${Math1Img})\n\n\n\n\n## Computing the Content Cost\n\nOne goal we should aim for when performing NST is for the content in generated image G to match the content of image C. To do so, we'll need an understanding of¬†**shallow versus deep layers**¬†:\n\n- The shallower layers of a ConvNet tend to detect lower-level features such as¬†*edges and simple textures*.\n- The deeper layers tend to detect higher-level features such as more¬†*complex textures and object classes*.\n\nWe choose a middle layer for the activations of the content image and the generated image and then calculate the content cost.\n\n![Math2Img](${Math2Img})\n\n\n## Computing the Style Cost\n\nThe Gram matrix captures the style of an Image at particular layer.\n\n![FilterUnRollingImg](${FilterUnRollingImg})\n![GramMatrixImg](${GramMatrixImg})\n\n![Math3Img](${Math3Img})\n\n\nOur goal will be to minimize the distance between the Gram matrix of the \"style\" image S and the Gram matrix of the \"generated\" image G.\n\n![Math4Img](${Math4Img})\n\nThe above cost function is defined for only 1 layer. In practice we will get better results if we use more layers. Each layer will be given weights (ùúÜ[ùëô]) that reflect how much each layer will contribute to the style.\n\nWe can combine the style costs for different layers as follows:\n\n![Math5Img](${Math5Img})\n\n## Defining the Total Cost to optimise\n\nFinally, we will create a cost function that minimises both the style and the content cost. The formula is:\n\n![Math6Img](${Math6Img})\n\n## Optimising to Generate Image from Noise\n\nFinally, we put everything together to implement Neural Style Transfer!\n\nHere's what the program will do:\n\n1. Load the content image\n2. Load the style image\n3. Randomly initialize the image to be generated\n4. Load the VGG19 model\n5. Compute the content cost\n6. Compute the style cost\n7. Compute the total cost\n8. Define the optimizer and learning rate\n\n\n## Implementation\n`\n\n\nexport const codeString = `import os\nimport sys\nimport scipy.io\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nimport pprint\n%matplotlib inline\n\n# laoding the pre-trained VGG model\npp = pprint.PrettyPrinter(indent=4)\nimg_size = 400\nvgg = tf.keras.applications.VGG19(include_top=False,\n                                  input_shape=(img_size, img_size, 3),\n                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\nvgg.trainable = False\npp.pprint(vgg)\n\n\n# viewing the content image\ncontent_image = Image.open(\"images/louvre.jpg\")\nprint(\"The content image (C) shows the Louvre museum's pyramid surrounded by old Paris buildings, against a sunny sky with a few clouds.\")\ncontent_image\n\n\n# compute content cost\ndef compute_content_cost(content_output, generated_output):\n    \"\"\"\n    Computes the content cost\n    \n    Arguments:\n    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n    \n    Returns: \n    J_content -- scalar that you compute using equation 1 above.\n    \"\"\"\n    a_C = content_output[-1]\n    a_G = generated_output[-1]\n\n    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n    \n    # Reshape 'a_C' and 'a_G'\n    a_C_unrolled = tf.reshape(a_C, shape=[m, -1, n_C])\n    a_G_unrolled = tf.reshape(a_G, shape=[m, -1, n_C])\n    \n    # compute the cost\n    J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)),axis=None)/(4*n_H* n_W* n_C)\n        \n    return J_content\n\n\n# getting the gram matrix that detects the style of the image\ndef gram_matrix(A):\n    \"\"\"\n    Argument:\n    A -- matrix of shape (n_C, n_H*n_W)\n    \n    Returns:\n    GA -- Gram matrix of A, of shape (n_C, n_C)\n    \"\"\"      \n\n    GA = tf.matmul(A,tf.transpose(A))\n\n    return G\n\n\n# compute style cost for a laye\ndef compute_layer_style_cost(a_S, a_G):\n    \"\"\"\n    Arguments:\n    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n    \n    Returns: \n    J_style_layer -- tensor representing a scalar value, style cost defined abov\n    \"\"\"\n\n\t\t# Retrieve dimensions from a_G\n    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n\n    # Reshape the tensors from (1, n_H, n_W, n_C) to (n_C, n_H * n_W)\n    a_S = tf.transpose(tf.reshape(a_S, shape=[n_H * n_W,n_C]))\n    a_G = tf.transpose(tf.reshape(a_G, shape=[n_H * n_W,n_C]))\n\n    # Computing gram_matrices for both images S and G\n    GS = gram_matrix(a_S)\n    GG = gram_matrix(a_G)\n\n    # Computing the loss\n    J_style_layer = tf.reduce_sum(tf.square(tf.subtract(GS,GG)))*((.5 / (n_H * n_W * n_C)) ** 2)\n    \n    return J_style_layer\n\n\n# adding style weights to multiple layers\nfor layer in vgg.layers:\n    print(layer.name)\n\nvgg.get_layer('block5_conv4').output\n\nSTYLE_LAYERS = [\n    ('block1_conv1', 0.2),\n    ('block2_conv1', 0.2),\n    ('block3_conv1', 0.2),\n    ('block4_conv1', 0.2),\n    ('block5_conv1', 0.2)]\n\n\n# compute multiple layer style cost\n\ndef compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n    \"\"\"\n    Computes the overall style cost from several chosen layers\n    \n    Arguments:\n    style_image_output -- our tensorflow model\n    generated_image_output --\n    STYLE_LAYERS -- A python list containing:\n                        - the names of the layers we would like to extract style from\n                        - a coefficient for each of them\n    \n    Returns: \n    J_style -- tensor representing a scalar value, style cost defined above\n    \"\"\"\n    \n    # initialize the overall style cost\n    J_style = 0\n\n    # Set a_S to be the hidden layer activation from the layer we have selected.\n    # The last element of the array contains the content layer image, which must not be used.\n    a_S = style_image_output[:-1]\n\n    # Set a_G to be the output of the choosen hidden layers.\n    # The last element of the list contains the content layer image which must not be used.\n    a_G = generated_image_output[:-1]\n    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):  \n        # Compute style_cost for the current layer\n        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n\n        # Add weight * J_style_layer of this layer to overall style cost\n        J_style += weight[1] * J_style_layer\n\n    return J_style\n\n\n# computing total cost function\n@tf.function()\ndef total_cost(J_content, J_style, alpha = 10, beta = 40):\n    \"\"\"\n    Computes the total cost function\n    \n    Arguments:\n    J_content -- content cost coded above\n    J_style -- style cost coded above\n    alpha -- hyperparameter weighting the importance of the content cost\n    beta -- hyperparameter weighting the importance of the style cost\n    \n    Returns:\n    J -- total cost as defined by the formula above.\n    \"\"\"\n\n    J = alpha*J_content + beta*J_style\n\n    return J\n\n\n# 1. load the content image\ncontent_image = np.array(Image.open(\"images/louvre_small.jpg\").resize((img_size, img_size)))\ncontent_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n\nprint(content_image.shape)\nimshow(content_image[0])\nplt.show()\n\n# 2. load the style image\nstyle_image =  np.array(Image.open(\"images/monet.jpg\").resize((img_size, img_size)))\nstyle_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n\nprint(style_image.shape)\nimshow(style_image[0])\nplt.show()\n\n# 3. Randomly Initialize the Image to be Generated - sligthly correlated with the content image\ngenerated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\nnoise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\ngenerated_image = tf.add(generated_image, noise)\ngenerated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n\nprint(generated_image.shape)\nimshow(generated_image.numpy()[0])\nplt.show()\n\n# 4. Load Pre-trained VGG19 Mode\ndef get_layer_outputs(vgg, layer_names):\n    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n\n    model = tf.keras.Model([vgg.input], outputs)\n    return model\n\ncontent_layer = [('block5_conv4', 1)]\n\nvgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)\n\ncontent_target = vgg_model_outputs(content_image)  # Content encoder\nstyle_targets = vgg_model_outputs(style_image)     # Style encoder\n\n# 5. Compute Total Cost\n# Assign the content image to be the input of the VGG model.  \n# Set a_C to be the hidden layer activation from the layer we have selected\npreprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\na_C = vgg_model_outputs(preprocessed_content)\n\n# Assign the input of the model to be the \"style\" image \npreprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\na_S = vgg_model_outputs(preprocessed_style)\n\ndef clip_0_1(image):\n    \"\"\"\n    Truncate all the pixels in the tensor to be between 0 and 1\n    \n    Arguments:\n    image -- Tensor\n    J_style -- style cost coded above\n\n    Returns:\n    Tensor\n    \"\"\"\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\ndef tensor_to_image(tensor):\n    \"\"\"\n    Converts the given tensor into a PIL image\n    \n    Arguments:\n    tensor -- Tensor\n    \n    Returns:\n    Image: A PIL image\n    \"\"\"\n    tensor = tensor * 255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor) > 3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return Image.fromarray(tensor)\n\n\n# 6. train step\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n@tf.function()\ndef train_step(generated_image):\n    with tf.GradientTape() as tape:\n        # Compute a_G as the vgg_model_outputs for the current generated image\n        a_G = vgg_model_outputs(generated_image)\n        \n        # Compute the style cost\n        J_style = compute_style_cost(a_S, a_G)\n        \n        # Compute the content cost\n        J_content = compute_content_cost(a_C, a_G)\n        # Compute the total cost\n        J = total_cost(J_content, J_style, alpha = 10, beta = 40)\n        \n    grad = tape.gradient(J, generated_image)\n\n    optimizer.apply_gradients([(grad, generated_image)])\n    generated_image.assign(clip_0_1(generated_image))\n\n    return J\n\n\n# generating\nepochs = 2501\nfor i in range(epochs):\n    train_step(generated_image)\n    if i % 250 == 0:\n        print(f\"Epoch {i} \")\n    if i % 250 == 0:\n        image = tensor_to_image(generated_image)\n        imshow(image)\n        image.save(f\"output/image_{i}.jpg\")\n        plt.show()\n\n\n\n# viewing the results\n# Show the 3 images in a row\nfig = plt.figure(figsize=(16, 4))\nax = fig.add_subplot(1, 3, 1)\nimshow(content_image[0])\nax.title.set_text('Content image')\nax = fig.add_subplot(1, 3, 2)\nimshow(style_image[0])\nax.title.set_text('Style image')\nax = fig.add_subplot(1, 3, 3)\nimshow(generated_image[0])\nax.title.set_text('Generated image')\nplt.show()\n`"],"names":["markdownContent","concat","NeuralStyleTransferConceptImg","Math1Img","Math2Img","FilterUnRollingImg","GramMatrixImg","Math3Img","Math4Img","Math5Img","Math6Img","codeString"],"sourceRoot":""}